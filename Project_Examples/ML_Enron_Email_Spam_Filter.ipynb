{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enron Email Spam Filtering Using a Naive Bayes Classification Method\n",
    "By Josh Wolfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup enviroment\n",
    "\n",
    "import random\n",
    "import os\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, movie_reviews\n",
    "from nltk.metrics import precision, recall, ConfusionMatrix\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "\n",
    "vec = DictVectorizer(sparse=False)\n",
    "\n",
    "wd = \"C:\\\\Users\\\\Josh\\\\Desktop\\\\mldtc\\\\Enron-data-set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in ham (not spam) and spam email files\n",
    "# Turns txt files into latin character based strings\n",
    "# Strings broken into word units (features) using tokenize\n",
    "# features are later used to train Naive Bayes\n",
    "\n",
    "# non-spam email\n",
    "ham = []\n",
    "ham_count = 0\n",
    "ham_emails = []\n",
    "\n",
    "#spam email\n",
    "spam = []\n",
    "spam_count = 0\n",
    "spam_emails = []\n",
    "\n",
    "# function turns words into features set to TRUE\n",
    "# TRUE words in each dictionary for spam and ham allows NB to compare word count for spam/ham\n",
    "\n",
    "\n",
    "\n",
    "def features(words):\n",
    "    feature_dictionary = dict([(word, True) for word in words])\n",
    "    return feature_dictionary\n",
    "\n",
    "\n",
    "\n",
    "# for loop loops through the file structure to get ham and spam txt files\n",
    "# sorts those txt files into the ham and spam lists\n",
    "# tokenizes the strings into words for Naive Bayes learning\n",
    "for directories, subdirectories, files in os.walk(wd):\n",
    "    if (os.path.split(directories)[1]  == 'ham'):\n",
    "        for filename in files:      \n",
    "            with open(os.path.join(directories, filename), encoding=\"latin-1\") as file:\n",
    "                data = file.read()\n",
    "                words = word_tokenize(data)\n",
    "                ham_emails.append(data)\n",
    "                ham.append((features(words), \"ham\"))\n",
    "                ham_count += 1\n",
    "    \n",
    "    if (os.path.split(directories)[1]  == 'spam'):\n",
    "        for filename in files:\n",
    "            with open(os.path.join(directories, filename), encoding=\"latin-1\") as file:\n",
    "                data = file.read()\n",
    "                words = word_tokenize(data)\n",
    "                spam_emails.append(data)\n",
    "                spam.append((features(words), \"spam\"))\n",
    "                spam_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize data for training and test sets\n",
    "\n",
    "# Concatinate both lists into one big dataset\n",
    "ham_spam = ham + spam\n",
    "\n",
    "# shuffles the tuples so that the dataset is not split on ham/spam\n",
    "# allows split to catch both ham and spam\n",
    "random.shuffle(ham_spam)\n",
    "\n",
    "# Set train/test split\n",
    "# 10% train 90% test\n",
    "split = int(len(ham_spam) * 0.01)\n",
    "\n",
    "X_train = ham_spam[:split]\n",
    "X_test = ham_spam[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Classification Accuracy :  80.23008478384614\n",
      "Multinomial Naive Bayes Classification Accuracy :  92.38143743071991\n",
      "Bernoulli Naive Bayes Classification Accuracy :  82.52494083106144\n",
      "Stochastic Gradient Descent (SGDC) Classification Accuracy:  92.9955960334342\n",
      "Support Vector Classification Accuracy :  92.75592438359448\n",
      "Nu Support Vector Classification Accuracy :  93.2472512657659\n",
      "Linear Support Vector Classification Accuracy :  93.97824979777705\n",
      "Logistic Regression Classification Accuracy :  94.82608825908505\n"
     ]
    }
   ],
   "source": [
    "# Train models\n",
    "\n",
    "NBclf = NaiveBayesClassifier.train(X_train)\n",
    "print(\"Original Naive Bayes Classification Accuracy : \", (nltk.classify.accuracy(NBclf, X_test)) * 100)\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(X_train)\n",
    "print(\"Multinomial Naive Bayes Classification Accuracy : \", (nltk.classify.accuracy(MNB_classifier, X_test)) * 100)\n",
    "\n",
    "BNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BNB_classifier.train(X_train)\n",
    "print(\"Bernoulli Naive Bayes Classification Accuracy : \", (nltk.classify.accuracy(BNB_classifier, X_test)) * 100)\n",
    "\n",
    "SGDC_classifier = SklearnClassifier(SGDClassifier(max_iter=1000))\n",
    "SGDC_classifier.train(X_train)\n",
    "print(\"Stochastic Gradient Descent (SGDC) Classification Accuracy: \", (nltk.classify.accuracy(SGDC_classifier, X_test)) * 100)\n",
    "\n",
    "SVC_classifier = SklearnClassifier(SVC())\n",
    "SVC_classifier.train(X_train)\n",
    "print(\"Support Vector Classification Accuracy : \", (nltk.classify.accuracy(SVC_classifier, X_test)) * 100)\n",
    "\n",
    "NuSVC_classifier = SklearnClassifier(NuSVC(max_iter=1000))\n",
    "NuSVC_classifier.train(X_train)\n",
    "print(\"Nu Support Vector Classification Accuracy : \", (nltk.classify.accuracy(NuSVC_classifier, X_test)) * 100)\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC(max_iter=1000))\n",
    "LinearSVC_classifier.train(X_train)\n",
    "print(\"Linear Support Vector Classification Accuracy : \", (nltk.classify.accuracy(LinearSVC_classifier, X_test)) * 100)\n",
    "\n",
    "LogReg_classifier = SklearnClassifier(LogisticRegression(max_iter=1000))\n",
    "LogReg_classifier.train(X_train)\n",
    "print(\"Logistic Regression Classification Accuracy : \", (nltk.classify.accuracy(LogReg_classifier, X_test)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to Naive Bayes Test message is : spam\n"
     ]
    }
   ],
   "source": [
    "# Test cases\n",
    "\n",
    "test_mess = \"Hey Alex! Would you like to buy a new car? I look forward to seeing you this weekend for our big sale!\"\n",
    "words = word_tokenize(test_mess)\n",
    "feats = features(words)\n",
    "print(\"According to Naive Bayes Test message is :\" ,NBclf.classify(feats))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
